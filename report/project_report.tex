\documentclass[12pt, A4]{article}
\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\graphicspath{ {.} }
\geometry{margin=1in}
\title{MPUM project - flappy bird reinforcement learning}
\author{Bartosz Bromblik \& Jacek Markiewicz}
\date{}
\begin{document}
\maketitle


\section{Wstęp}
\textit{"Flappy bird"} to prosta gra z długą historią irytowania wszystkich graczy. Mimo swojej prostoty, wcale nie jest prosta. A przynajmniej dla człowieka. A czy tak samo jest dla maszyny?
\newline
Celem projektu jest sprawdzenie skuteczności domowej roboty implementacji kilku algorytmów uczenia przez wzmocnienie (ang. RL - Reinforcement Learning), tu:
\begin{itemize}
	\item Q-learning,
	\item Sieć neuronowa,
	\item Algorytm genetyczny.
\end{itemize}
Do raportu załączone jest (chaotyczne) repozytorium z zaklepanymi algorytmami, ich wynikami oraz samą grą.


\section{Opis gry}
Jesteśmy ptakiem (żółtą kulką) i chcemy dolecieć jak najdalej. Ale na naszej drodze jest pełno rur (to zielone). Na szczęście są w nich dziury, przez które możemy spróbować przelecieć. I to tyle co można powiedzieć o celu gry.
\newline
Całość kontroli to klikanie \textit{spacji}, które powoduje skok ptaka. Poza tym \textit{Esc} wyłącza grę, \textit{R} (niekoniecznie duże) restartuje rozgrywkę. Każdy inny przycisk działa jako pauza (lub tę pauzę cofa).
\newline\newline
Gra jest mniej więcej w pełni konfigurowalna. Parametry można znaleźć w pliku \textit{game\_config.py}.
\newline
Ptak zawsze zaczyna na wysokości połowy mapy, z prędkością poziomą równą $0$. Grawitacja jest stała, prędkość pozioma też. Skok ustawia prędkość pionową na tę samą wartość.
\newline
Grubość (pozioma) rury jest stała. Odległości między rurami są brane z rozkładu normalnego. Środek dziury jest brany z rozkładu jednostajnego. Promień dziury jest brany z rozkładu normalnego i z każdą kolejną rurą maleje (tempo zależy od poziomu trudności).
\newline\newline
Hiperparametr \textit{HARD} określa poziom trudności gry (\textit{False} to łatwy, \textit{True} to trudny). 
\newline\newline
Silnik gry nie ma wbudowanego zegara. Dzięki temu można trenować modele szybciej niż w czasie rzeczywistym. A w trybach do grania jest oddzielny zegar, który pilnuje odpowiedniego framerate'u.
\newline
Moduły do wyświetlania gry jest napisany w pygame'ie.


\section{Rozgrywka}
Pliki \textit{play\_*.py} to ta grywalna część projektu. Każdy odpowiada któremuś z algorytmów (sposób implementacji może sę różnić między plikami) poza plikiem \textit{play\_yourself.py}, gdzie nie ma żadnego wspomagania. Tak, sterowanie działa wszędzie i algorytmom można przeszkadzać.


\section{Ogólnie o uczeniu}
Podczas gry, na górze okna wyświetlają się na czerwono 3 tuple liczb. Oznaczają one kolejno:
\begin{itemize}
	\item obecną (uogólnioną) stratę, czyli odległość pozioma minus różnica poziomów środka ptaka i środka następnej dziury, używana do oceny modelu. Często zwana również \textbf{wynikiem}.
	\item stan, czyli (pozycję pionową, prędność pionową, pozycję dolnego końca następnej dziury, pozycję górnego końca następnej dziury, odległość poziomą do następnej rury). Przy czym to ostatnie jest liczone od prawego końca ptaka do lewego końca rury. Dopiero gdy ptak w całości minie linię końca rury, zmieniana jest "następna rura".
	\item (ilość miniętych rur, status), gdzi to drugie to $1$ gdy dalej żyjemy i $0$ gdy już nie.
\end{itemize}
Stan świata jaki widzi model to dokładnie ta środkowa tupla.


\section{Uczenie nie maszynowe}
Jako kontekst powiem, że pomimo wielu prób, nie udało mi się zdobyć wyniku powyżej 7 (samej siódemki też zresztą nie).


\section{Q-learning}
Pierwszą z zastosowanych metod była metoda tzw. Q-learningu. Algorytm ten jest bardzo ogólny i robi niewiele założeń. Traktujemy nasz rozważany świat jako zbiór \textit{stanów}, między którymi jesteśmy w stanie przechodzić przy użyciu \textit{akcji} (skok / nie-skok).  \\
	Teraz będziemy starać nauczyć funkcji $Q: stan \times akcja \rightarrow \mathbb{R}$ (''Q'' od ang. \textit{quality}) - która dla każdej potencjalnej akcji w danym stanie da nam ''jakość'' danego ruchu. Decyzję ostateczną podejmiemy zależnie od tego, która z akcji będzie miała dla danego stanu odpowiednio większą wartość Q. \\	
	Będziemy to robić następująco - Q początkowo jest dowolnie zinicjalizowane. Następnie przeprowadzamy rozgrywkę. W każdym momencie czasowym $t$ naszej rozgrywki możemy ją opisać przez stan $S_t$. Podejmujemy w każdym momencie akcję $A_t$ zależnie od Q. W ten sposób przechodzimy do stanu $S_{t+1}$ oraz otrzymujemy pewną \textit{nagrodę} $R_{t+1}$. Następnie aktualizujemy funckję Q w oparciu o poniższu wzór:
	\[	Q(S_t, A_t) = (1-\alpha)\cdot Q(S_t, A_t) + \alpha \cdot \left( R_{t+1} + \gamma \max_a Q(S_{t+1}, a) \right)	\]
	gdzie $\alpha$ - szybkość uczenia się (\textit{learning rate}) oraz $\gamma$ - współczynnik dyskontowy (\textit{discount factor}).
	Czyli aktualizujemy Q w oparciu o oczekiwaną nagrodę w przyszłości. \\
	W trakcie implementacji tego rozwiązania napotkaliśmy jednak niejedną komplikację. \\

	Otóż agorytm ten ma jedną, dość istotną wadę. Implementowany najczęściej jest on, ze względu na brak lepszego sposobu, poprzez fizyczne trzymanie tablicy Q indeksowanej stanami i akcjami. To narzuca jednak to, że zbiory stanów i akcji muszą być skończone. O ile akcje nie są problemem u nas, o tyle stany już sprawiają pewnego rodzaju zagwostkę. Stan gry, który dla przypomnienia jest w postaci krotki zawierającej:
	\begin{itemize} 
	\item wysokość ptaka
	\item prędkość pionową ptaka
	\item	odległość poziomą do następnej rury
	\item odległość pionowa do następnej rury
	\end{itemize}
	trzeba zkwantyzować. Ostatecznie, w wyniku wielu prób i błędów, każda wartość z owej krotki dostała odpowiednio 16, 11, 25, 14 unikalnych, dyskretnych wartości. Taka dyskretyzacja wartości z jednej strony nie traciła aż tyle informacji, a z drugiej przestrzeń stanów akceptowalnej wielkości ($16\times11\times25\times14 = 61600$)\\

	Ważna w algorytmie jest również \textit{eksploracja}. Otóż, jeśli nie będziemy nigdy widzieć / eksplorować pewnych stanów gry, to po ich napotkaniu nasz algorytm nie będzie działał dobrze - za mało razy byliśmy w danym stanie i wartość Q dla niego nie jest jeszcze pewna. Dlatego też na początku treningu staramy się wykonywać naszym agentem losowe akcje. W dalszej fazie treningu proporcja losowych akcji systematycznie spada do bardzo małej - ale niezerowej - wartości.    

	Istotne było również dobranie dobrej funkcji kosztu. Początkowo nagradzała jedynie za przeżywanie, oraz bardzo karciła za przegrywanie. Jednakże nauka szła opornie. Dlatego dodałem małą modyfikację do funkcji kosztu - nagradzałem dodatkowo za przebywanie na wysokości w pobliżu dziury w następnej rurze. Efekt był zadowalający - algorytm szybciej i sprawniej się uczył. \\
	Model oparty o Q-learning wytrenowany na przestrzeni 20000 rozgrywek jest w stanie pokonać średnio \textbf{4.662} rur.	\\
	Uruchamiając \textit{play\_q.py} można oglądać zmagania tego modelu (po jego uprzednim wytrenowaniu, co nie trwa długo). W czasie rozgrywki można zauważyć, że model radzi sobie całkiem dobrze, jednak nie idealnie. Ciężko stwierdzić, na ile niedokładności te wynikają z niedotrenowania a na ile z błędów w reprezentacji stanów (tj. utratę informacji). 

	\subsection{SARSA}
	Zaprezentowany sposób aktualizacji tablicy Q nie jest jedynym możliwym. Algorytm \textit{SARSA} (\textit{state–action–reward–state–action}) bazuje na poniższej metodzie aktualizacji:
	\[	Q(S_t, A_t) = (1-\alpha)\cdot Q(S_t, A_t) + \alpha \cdot \left( R_{t+1} + \gamma Q(S_t, A_t) \right)	\]
	Reszta algorytmu pozostaje niezmieniona względem Q-learningu. Dzięki zastosowaniu tej prostej zmiany, (oraz podobnie jak wcześniej wytrenowniu na przestrzeni 20000 rozgrywek) jesteśmy osiągnąć rezultaty na poziomie \textbf{5.389} przeskoczonych rur.\\

	Niestety, ciężko nam uzyskać idealne rezultaty - kwantyzacja stanów idealna nie jest co przekłada sie na niedokładności w reprezentacji rzeczywistego stanu oraz co za tym idzie okazjonalną niespodziewaną śmiercią.\\
	Do podziwiania rozgrywek tego modelu załączamy \textit{play\_sarsa.py}. Zachowanie dosyć podobne do poprzedniego z modeli.

\section{Sieć neuronowa}
Postalowiliśmy również spróbować zaimplementować sieć neuronową, by i ta zmierzyła się z naszą grą. \\
	Koniec końców zdecydowaliśmy się na tzw. \textit{DQN} (Deep Q Network). Czyli tak naprawdę będziemy znowu starali się wyznaczyć funkcję ''jakości'' Q, ale tym razem przy użyciu sieci neuronowej.
	Sieć posiada wymiary [4,16,16,2], gdzie cztery wejścia odpowiednio:
	\begin{itemize}
	\item wysokość ptaka
	\item prędkość pionowa ptaka
	\item odległość pozioma do następnej rury
	\item odległość pionowa do następnej rury
	\end{itemize}
	Wartości te na start są z grubsza w przedziale [0,1], nie ma więc potrzeby żadnego skalowania. \\
	Warstwy ukryte były w pełni połączone oraz używały funkcji aktywacyjnych LeakyReLU. Oba z wyjść odpowiadają wartościom odpowiedno $Q(s, 0), Q(s, 1)$ - jakości ruchu ''nie-skoku'' oraz ''skoku'' będąc w stanie $s$. \\
	Jednakże sam proces uczenia był \textbf{bardzo} niestabilny. W celu jego ustabilizowania zastosowaliśmy parę technik, często używanych w DQN:
	\begin{itemize}
	\item Bufor z ''powtórkami'' - trenowanie modelu w oparciu jedynie o najnowsze doświadczenia nie jest dobrym pomysłem - spora część tych zdarzeń będzie zależna od siebie wzajemnie co może niegatywnie wpłynąć na proces uczenia. Zaradzić temu można tworząc spory bufor stałego rozmiaru, gdzie będa trafiać kolejne ''doświaczenia'' (w postaci krotek $(S_t, A_t, R_{t+1}, S_{t+1})$), a stare będą po pewnym czasie usuwane. Następnie, chcąc nauczyć otrzymać próbkę danych do nauki losujemy wartości z naszego bufora.
	\item Sieć ''referencyjna'' - do oceny oczekiwanej nagrody w przyszłości używamy tej samej funckji Q co dla wyliczania aktualnej nagrody. Może to się wiązać z dużą niestabilnością. Rozwiązaniem jest wprowadzenie drugiej sieci ''referencyjnej'', którą będziemy aktualizować rzadziej od głównej (np. co 500 rozgrywek zamiast co każdą) i to ją będziemy używać do wyznaczania \textit{przyszłych} wartości $Q(S_{t+1}, a)$.  
	\end{itemize}
	Oraz zastosowaliśmy kilka pomniejszych, ale wciąż istnotnych usprawnień:
	\begin{itemize}
	\item Również tu była potrzeba ''eksploracji'' - na początku przymuszamy nasz algorytm do wykonywania losowych akcji, by nagromadzić różnorakie doświadczenia. Potem coraz bardziej pozwalamy modelowi podejmować bardziej świadome decyzje.
	\item Funkcja kosztu została zmieniona - oryginalnie funkcją kosztu była ilość ominiętych rur. Jest to sensowna miara jakości modelu, aczkolwiek jest ona zbyt małomówna dla naszego modelu. \\
	Dlatego funkcję tę zmieniłem tak, że po każdej przeżytej sekundzie algorytm jest nagradzamy punktem, a w przypadku śmierci otrzymuje ogromną karę. Niestety i ta funkcja okazała się zbyt małowówna i trening przebiegał bardzo opornie. \\
	Ostatecznie funkcją, która się sprawdziła dosyć dobrze była taka, która nagradzała za przeżycie, bardzo karciła za śmierć oraz dodatkowo nagradzała ptaka za posiadanie współrzędnej Y-owej podobnej do wsp. Y-owej następnej dziury w rurze. 
	\item Od czasu do czasu pojawiały się duże niestabilności natury numerycznej w samej aktualizacji wag sieci. Powodem był gradient, który czasem osiągał nienaturnalnie duże wartości. W celu temu zaradzenia ograniczaliśmy wartości gradientu tak, by nigdy nie osiągały zbyt dużych wartości. 
	\end{itemize}
	Po rozprawieniu się ze wszystkimi wspomnianymi przeciwnościami udało nam się wytrenować dość dobrze działający model. Co prawda trening był dosyć czasochłonny (mimo małej sieci wciąż zajął na laptopie około godziny), ale trud był tego wart. Nasz ptak, posługując się modelem korzystającym z opisanej sieci neuronowej jest w stanie pokonywać średnio \textbf{6.367} rur. \\
	Aby kibicować sieci neuronowej podczas jej zmagań wystarczy uruchomić \textit{play\_nn.py}. W związku z bardzo długim treningiem owej sieci, sieć ta na ten moment jest ładowana z \textit{model.pickle}. Można również wytrenować sieć od zera odkomentowując odpowiednie linie w \textit{play\_nn.py}.

\section{Algorytm ewolucyjny}
Ogólny zamysł metody jest taki, żeby na początku wygenerować losowo działające modele, a następnie wybierać najlepsze, usuwać najgosze, mutować to co zostało i ponawiać pętle.
\newline
W obecnej implementacji wszystko opiera się o arbitralnie wybraną sieć neuronową o wymiarach $[5, 5, 2]$, czyli z tylko jedną ukrytą warstwą. Funkcja aktywacji to ReLU, a wynik sieci jest mielony przy użyciu softmax'a. Opisany kod znajduje się w pliku \textit{evolutionary.py}.
x
\subsection{Proces uczenia}
Na początku parametry wszystkich modeli są inicjalizowane rozkładem normalnym $\mathcal{N}(0, 10^{-1})$.
Następnie w pętli $\#epok$-krotnie:
\begin{itemize}
	\item generowany jest układ rur (mapa), na którym oceniane będą modele,
	\item każdy model jest oceniany,
	\item 10\% najlepszych modeli pozostaje w populacji nieulegając zmianie,
	\item 30\% najlepszych modeli (w czym powyższe) trafia do populacji w trzech kopiach, odrobinę zmienionych. Tzn. do ich parametrów dodawane są wartości brane z $\mathcal{N}(0, 10^{-4})$,
	\item zapisywane są wyniki (nie wpływa na działanie algorytmu).
\end{itemize}
Należy zauważyć, że rozmiar populacji nie ulega zmianie.
\newline
Na koniec każdy pozostały w populacji model jest oceniany ja 10 konfiguracjach mapy (każdy model na każdej z tych map) i wybierany jest ten działający średnio najlepiej.

\subsection{Rezultaty}
Pomimo relatywnie prostej implementacji, bez krzyżowania modeli, przekazywania genów itd., wyniki są całkiem niezłe. Zapisane są trzy wyniki, dla ($\#epok$, $rozmiar \ populacji$) kolejno: ($100$, $100$), ($200$, $200$), ($400$, $400$), o czasach trenowania rzędu: minuta, pare minut, parenaście minut.
\newline
\includegraphics[width=8cm, height=6cm]{evo\_example\_100\_100}
\includegraphics[width=8cm, height=6cm]{evo\_example\_200\_200}
\newline
\includegraphics[width=8cm, height=6cm]{evo\_example\_400\_400}
\newline
Zdecydowanie warto zauważyć, że wyniki mocno zależą od mapy. Uśrednione wyniki oscylują w okolicy 6 z hakiem, choć trafiają się również wyśmienite układy rur pozwalające na wyniki około 9.
\newline
Na trzecim rysunku widać, że wyniki całości populacji niewiele odbiegają od wyników najlepszych 10\%, co może być spowodowane fizycznymi ograniczeniami mapy.

\subsection{Dokładne rezultaty}
Modele będące wynikami przebiegów, które wygenerowały powyższe rysunki, dostępne są w pliku \textit{models\_evolutionary.py}. Plik ten można odpalić by dokładniej ocenić osiągnięte sieci. Po takim właśnie przetestowaniu, na $10^4$ mapach, wyniki prezentują się następująco:
\begin{itemize}
	\item Próba mała, ($100$, $100$): $7.0758$.
	\item Próba średnia, ($200$, $200$): $7.4886$.
	\item Próba duża, ($400$, $400$): $7.0753$.
\end{itemize}
Wszystkie wyniki są całkiem dobre, zdecydowanie lepsze niż nawet rekordy co poniektórych autorów. Należy pamiętać, że każde z powyższych to średni wynik z wielu przebiegów \textbf{JEDNEGO} modelu, co niewiele jest w stanie powiedzieć o rzeczywistej przewadze tych czy innych hiperparametrów.
\newline\newline
Ten środkowy model jest dołączony również w pliku \textit{play\_evo.py}, gdzie można pooglądać, jak sobie gra.
\newline
Jego taktyka nie wydaje się zbyt skomplikowana. Model utrzymuje ptaka na wysokości środka kolejnej dziury i zdaje się nie myśleć zbyt wiele więcej, choć nie są to bardzo uważne obserwacje.


\section{Podsumowanie}
Przyszła pora na ostateczne porównanie i omówienie przedstawionych metod. W celu porównania sprawności danej metody, przeprowadzamy 100 rozgrywek w czasie których ptak korzysta z odpowieniego modelu do podejmowania decyzji o skoku / nie-skoku. Po przetestowaniu w tenże sposób każdej z metod otrzymamy następujące wyniki:
	
	\begin{center}	
	\begin{tabular}{||c c||}
	\hline
	 $metoda$ & wynik \\ [0.5ex]
	\hline\hline
	 Q-learning & 5.364 \\
	 Q-learning(SARSA) & 5.597 \\
	 Sieć neuronowa & 6.455 \\
	 Ewolucyjny (mała próba) & 7.138 \\
	 Ewolucyjny (średnia próba) & \textbf{7.178} \\
	 Ewolucyjny (duża próba) & 6.894 \\ [1ex]
	 \hline
	\end{tabular}
	\end{center}

	Najgorzej (choć i tak satysfakcjonująco) poradziła sobie metoda oparta na Q-learning'u. Jest ona dosyć prostą metodą, więc fakt że poradziła sobie najgorzej nie jest niespodziewany. Fakt, że musimy poświęcać część informacji poprzez dyskretyzację danych daje się we znaki. \\
	Dochodzimy tu też do wniosku, że Q-learning w wersji SARSA sprawdza się u nas lepiej od wersji standardowej (nie jest to błąd pomiarowy, wynik okazywał się lepszy w kilku niezależnych próbach).

	Model oparty na sieciach neuronowych też radzi sobie bardzo dobrze. Uważamy, że przy lepszym doborze parametrów, dłuższym treningu tudzież zastosowaniu gotowych rozwiązań miałby on szanse przebić algorytm ewolucyjny pod względem uzyskanego wyniku. \\

	Ku naszemu zaskoczeniu najlepiej poradził sobie model stosujący algorytm genetyczny. Pobił on swoją wydajnością nawet mocno dopracowany model używający sieci neuronowej. Podejrzewamy, że może to wynikać z faktu, iż nasz problem jest na tyle prosty, że ''wylosowanie'' dobrej strategii jest bardziej prawdopodobne od uzyskania jej na drodze treningu sieci, który to często nie jest stabilny. \\
\end{document}